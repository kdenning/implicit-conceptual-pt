---
title: "Implicit Conceptual PT Data Analysis (for SAP requirement)"
author: "Kathryn Denning"
date: "2/14/2020"
output: 
  html_document:
    code_folding: "hide"
    toc: true
    toc_float: true
---

# Introduction to study

This study looked at whether people consider the conceptual perspectives (e.g., likes) of others without conscious awareness, and if this depends on whether the target individual has the same visibility of the target stimuli as them. Participants were asked to respond about how much they liked the stimuli after being trained on the targets' likes and dislikes. Implicit conceptual perspective taking would be found if people are slower to respond on inconsistent trials in comparison to consistent trials.

**Variable names included in the analysis:**

* **consistent**: Whether the participant had the same conceptual perspective as the target or not. Helemert contrasts were applied to this categorical variable. 
  + consistent: perspectives about whether or not they liked the stimuli were the same
  + inconsistent: perspectives were different
  + unknown: the participant does not know the target's perspective
  + dislike: the participant dislikes the stimuli; included as a comparison so that participants were not only responding "like" the entire time
* **visibility**: Whether the target individual can see the food stimuli or not.
* **exp_resp.rt **: Reaction time on trials in seconds.


```{r setup, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("psy")
#install.packages("irr")
#install.packages("emmeans")
#install.packages("sjPlot")
#install.packages("effects")
#install.packages("fs")
library(rio)
library(here)
library(tidyverse)
library(magrittr)
library(janitor)
library(lme4)
library(psy)
library(irr)
library(psych)
library(sjPlot)
library(emmeans)
library(effects)
library(here)
library(fs)

#Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

```{r combining datasets and cleaning, include = FALSE}
# Getting file names for each individual participants datasets into one character vector
files <- dir_ls(here::here("Data"), glob = "*.csv")

# Reading one dataset in to figure out how I want to clean it
sap1 <- readr::read_csv(file = files[1],
                col_names = TRUE)

# Practice cleaning on one dataset
sap1_sub <- sap1 %>% select(participant, expBlock.thisN, expTrials.thisN, 
                            facing_exp, person_exp, food_exp, showL_R, 
                            consistent, exp_resp.keys, exp_resp.rt) %>% 
  na.omit() %>% 
  mutate(trial_num = as.numeric(row_number()),
         consistent = as.factor(consistent), 
         exp_resp.keys = as.factor(exp_resp.keys),
         food_exp = as.factor(food_exp),
         visibility = as.factor(ifelse(facing_exp == showL_R, 
                                       "same", 
                                       ifelse(facing_exp != showL_R, 
                                              "diff", NA))),
         error = as.factor(ifelse(exp_resp.keys == "up" & food_exp == "y", 
                                  "Error",
                                  ifelse(exp_resp.keys == "down" 
                                         & food_exp == c("g", "d"),
                                         "Error", 
                                         ifelse(exp_resp.keys == "up" 
                                                & food_exp == c("g", "d"), 
                                                "Correct",
                                                ifelse(exp_resp.keys == "down" 
                                                       & food_exp == "y", 
                                                       "Correct", NA))))))
                                  
# Writing a function to clean all the datasets using practice from above
## Only selecting variables we want from the larger Psychopy dataset
## Creating variables for trial number and for whether the target has the same visibility as the participant
clean_sap <- function(file) {
                    sap <- read_csv(file)
                    sap %>% 
                      #selecting the variables relevant to analysis from the dataset
                    select(participant, expBlock.thisN, expTrials.thisN, 
                            facing_exp, person_exp, food_exp, showL_R, 
                            consistent, exp_resp.keys, exp_resp.rt) %>% 
                      na.omit() %>% 
                      # making variables correct form
                      mutate(trial_num = as.numeric(row_number()),
                             consistent = as.factor(consistent),
                             exp_resp.keys = as.factor(exp_resp.keys),
                             food_exp = as.factor(food_exp),
                             # creating a variable for target's ability to see stimuli
                             visibility = as.factor(ifelse(facing_exp == showL_R,
                                                           "Visible", 
                                                           ifelse(facing_exp != showL_R, 
                                                                  "Not Visible", NA))),
                             # making it easier to understand the levels of consistency
                             consistent  = recode(consistent, 
                            `c` = "Consistent",
                            `i` = "Inconsistent",
                            `u` = "Unknown",
                            `y` = "Dislike")) %>% 
                      # filtering out practice block and first trial in each block
                      filter(expBlock.thisN != "0" | expTrials.thisN != "0")
}

# Drop "6_APCT" files because participant never made it to experimental portion and dataset is in different format due to that
files <- grep("/6_ACPT", files, value = TRUE, invert = TRUE)

# Apply function to list of datasets to cycle through and create one overall dataset
data <- map_df(files, clean_sap, .id = "column_names")

# Adding error variable and effects coding visibility
dat_error <- data %>%  
  mutate(error = with(data, ifelse(exp_resp.keys == "up" 
                                   & food_exp == "y", "Error", 
                                   ifelse(exp_resp.keys == "down" 
                                          & food_exp == c("g", "d"), "Error",
                                          "Correct"))),
         visibility_effects = recode(visibility, 
                                      `Visible` = "-.5",
                                      `Not Visible` = ".5"))

# Making error a factor variable
dat_error %<>% mutate(error = as.factor(error)) %>% 
  na.omit()

# Read in and merge demographic information into overall dataset
dem <- read.csv("ACPT_dem.csv", header = TRUE) 

data_comb <- left_join(dat_error, dem, by = "participant")

# Checking where there is na data
colnames(data_comb)[colSums(is.na(data_comb)) > 0]

# Creating a separate dataset for analyses from what will be used for demographics to not exclude people who have na's in dem data since dem data was optional
dat_analysis <- data_comb %>% 
  select(participant, trial_num, expBlock.thisN, food_exp, consistent, 
         exp_resp.rt, exp_resp.keys, visibility, visibility_effects, error)
```

# Model predicting reaction time

## Checking for outliers and filtering responses over 3 seconds

```{r rt outliers over 3 sec}
# Checking outliers in reaction time overall
hist_rt <- dat_analysis %>% 
  select(participant, exp_resp.rt) %>% 
  unique()
hist(hist_rt$exp_resp.rt, xlim = c(0, 60), breaks = 150)

# Removing trials that took over 3 seconds to respond
data_rt <- dat_analysis %>% 
  filter(exp_resp.rt < 3.0)

# Re-checking for outliers in reaction time overall
hist_rt_3sec <- data_rt %>% 
  select(participant, exp_resp.rt) %>% 
  unique()
hist(hist_rt_3sec$exp_resp.rt, xlim = c(0, 5), breaks = 50)

# percent removed over 3 seconds
(1 - nrow(data_rt)/nrow(dat_analysis))*100
```

## Descriptives before removing error trials

### Mean and SD for reaction time by consistency condition

```{r mean aggregation consist before removing errors}
# aggregating responses per participant
data_aggregated_werror <- data_rt %>% 
  group_by(participant, consistent, visibility) %>% 
  summarize(rt_pagg = mean(exp_resp.rt))

consistency_werror <- data_aggregated_werror %>% 
  select(participant, consistent, rt_pagg) %>% 
  na.omit() %>% 
  group_by(consistent) %>% 
  summarize(mean = mean(rt_pagg),
            sd = sd(rt_pagg),
            se = sd(rt_pagg)/sqrt(length(rt_pagg)))
consistency_werror 
```

### Mean and SD for reaction time by visibility condition

```{r mean aggregation visible before removing errors}
visibility_werror <- data_aggregated_werror %>% 
  select(participant, visibility, rt_pagg) %>% 
  na.omit() %>% 
  group_by(visibility) %>% 
  summarize(mean = mean(rt_pagg),
            sd = sd(rt_pagg),
            se = sd(rt_pagg)/sqrt(length(rt_pagg)))
visibility_werror
```

### Overall variable descriptives

```{r mean aggregation overall before removing errors}
data_aggregated_werror %>% 
  select(visibility, consistent, rt_pagg) %>% 
  describe()
```

## Descriptives after removing error trials

### Mean and SD for reaction time by consistency condition

```{r mean aggregation consist after removing errors}
data_aggregated_nerror <- data_rt %>% 
  filter(error != "Error") %>%
  droplevels(data_rt$error) %>% 
  na.omit() %>% 
  group_by(participant, consistent, visibility) %>% 
  summarize(rt_pagg = mean(exp_resp.rt))

consistency_nerror <- data_aggregated_nerror %>% 
  select(participant, consistent, rt_pagg) %>% 
  na.omit() %>% 
  group_by(consistent) %>% 
  summarize(mean = mean(rt_pagg),
            sd = sd(rt_pagg),
            se = sd(rt_pagg)/sqrt(length(rt_pagg)))
consistency_nerror 
```

### Mean and SD for reaction time by visibility condition

```{r mean aggregation visible after removing errors}
visibility_nerror <- data_aggregated_nerror %>% 
  select(participant, visibility, rt_pagg) %>% 
  na.omit() %>% 
  group_by(visibility) %>% 
  summarize(mean = mean(rt_pagg),
            sd = sd(rt_pagg),
            se = sd(rt_pagg)/sqrt(length(rt_pagg)))
visibility_nerror
```

### Overall variable descriptives

```{r mean aggregation overall after removing errors}
data_aggregated_nerror %>% 
  select(visibility, consistent, rt_pagg) %>% 
  describe()
```

## Percent of error trials vs correct trials

```{r percent error}
# Percent error
data_rt %>% 
  select(error) %>% 
  group_by(error) %>% 
  count() %>% 
  mutate(percent = n/(3509+57222)*100)
```

## Anova analysis

### Contrasts

```{r setup and contrasts}
# Removing dislike condition from anaysis as it was just a methodological variation not theoretically related to analysis
# Removing errors from analysis
anova_rt_data <- data_rt %>% 
  filter(error != "Error") %>% 
  filter(consistent != "Dislike") %>%
  droplevels(data_rt$error) %>% 
  droplevels(data_rt$consistent) %>% 
  na.omit() %>% 
  group_by(participant, consistent, visibility) %>% 
  mutate(rt_pagg = mean(exp_resp.rt)) %>% 
  select(participant, consistent, visibility, visibility_effects, rt_pagg) %>% 
  unique() %>% 
  na.omit()

CIvU <- c(-1, -1, 2) 
CvI <- c(-1, 1, 0) 
ConCodes <- cbind(CIvU, CvI)
contrasts(anova_rt_data$consistent) <- ConCodes
contrasts(anova_rt_data$consistent)
```

*Visibility was effects coded*

### Results

```{r rt anova results}
# Running the repeated measures anova
model1 <- with(anova_rt_data, 
               aov(rt_pagg ~ (visibility_effects*consistent) + 
                     Error(participant/(visibility_effects*consistent))))
summary(model1)
```

## Plots

```{r rt consistent no error plot}
ggplot(consistency_nerror , aes(x = consistent, y = mean, fill = consistent)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_fill_viridis_d() +
  theme(legend.position = "none") +
  labs(title = "Reaction time predicted by perspective consistency",
       subtitle = "Response error has been removed",
       x = "Perspective consistency condition",
       y = "Reaction time (seconds)") +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = .2,
                position = position_dodge(.9)) 
```

```{r rt visibility no error plot}
ggplot(visibility_nerror , aes(x = visibility, y = mean, fill = visibility)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_fill_viridis_d() +
  theme(legend.position = "none") +
  labs(title = "Reaction time predicted by visibility",
       subtitle = "Response error has been removed",
       x = "Visibility condition",
       y = "Reaction time (seconds)") +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = .2,
                position = position_dodge(.9)) 
```

# Model predicting error

## Helmert ontrasts for consistency

```{r error setup}
# Organizing data to include bothlevels of error but get rid of the dislike condition
anova_error_data <- data_rt %>% 
  select(participant, error, visibility_effects, visibility, consistent) %>% 
  filter(consistent != "Dislike") %>%
  droplevels(data_rt$consistent) %>% 
  unique() %>% 
  na.omit()

# Applying helmert contrasts to consistent; visibility is already effects coded
contrasts(anova_error_data$consistent) <- ConCodes
contrasts(anova_error_data$consistent)
```

*Visibility was effects coded*

## Logistic regression results predicting error

```{r}
# Running logistic regression predicting error
model2 <- glm(error ~ visibility_effects*consistent, 
              data = anova_error_data, 
              na.action = "na.exclude", 
              family = "binomial")
summary(model2)
```

## Odds of logistic regression coefficients (aka exponentiated coefficients)

```{r odds}
# Getting exponentiated coefficients (odds)
exp(coef(model2))
```

```{r cross table, include = FALSE}
# Get predicted probabilities:
predicted_probs <- predict(model2, type="response")

glm_pred <- ifelse(predicted_probs > 0.5, "Correct", "Error")

attach(anova_error_data)
table(glm_pred, error)
# Get a classification table:
cutoff <- .5 # setting a cut-off

```

# Demographics

## Number of participants with demographic data

```{r participant number}
data_comb %>% 
  select(participant) %>% 
  unique() %>% 
  nrow()
```

## Gender

```{r gender}
data_comb %>% 
  select(participant, gender) %>% 
  na.omit() %>% 
  unique() %>% 
  group_by(gender) %>% 
  count()
```

## Race

```{r race}
data_comb %>% 
  select(participant, race) %>% 
  na.omit() %>% 
  unique() %>% 
  group_by(race) %>% 
  count() %>% 
  mutate(percent = n/94*100)
```

## Age

```{r age}
data_comb %>% 
  select(participant, age) %>% 
  na.omit() %>% 
  summarize(mean = mean(age),
            sd = sd(age))
```

## Class level

```{r class level}
data_comb %>% 
  select(participant, class_level) %>% 
  na.omit() %>% 
  unique() %>% 
  group_by(class_level) %>% 
  count()
```

## Parent's education

```{r parents education}
data_comb %>% 
  select(participant, parents_edu) %>% 
  na.omit() %>% 
  unique() %>% 
  group_by(parents_edu) %>% 
  count()
```

## Native language

```{r native language}
data_comb %>% 
  select(participant, english_native) %>% 
  na.omit() %>% 
  unique() %>% 
  group_by(english_native) %>% 
  count()
```

## Born in the US

```{r birth country}
data_comb %>% 
  select(participant, us_birth_country) %>% 
  na.omit() %>% 
  unique() %>% 
  group_by(us_birth_country) %>% 
  count()
```

## Raised in the US

```{r country raised in}
data_comb %>% 
  select(participant, us_raised) %>% 
  na.omit() %>% 
  unique() %>% 
  group_by(us_raised) %>% 
  count()
```